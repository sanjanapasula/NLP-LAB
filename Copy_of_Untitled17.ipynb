{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESlxB8voEVMX",
        "outputId": "61eb4e74-c651-4828-d0a7-c869946d38e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    the ukrainian president says the country will ...\n",
            "1    jeremy bowen was on the frontline in irpin as ...\n",
            "2    one of the worlds biggest fertiliser firms say...\n",
            "3    the parents of the manchester arena bombings y...\n",
            "4    consumers are feeling the impact of higher ene...\n",
            "Name: cleaned_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"bbc-news.csv\")\n",
        "texts = df[\"description\"].dropna().astype(str)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"cleaned_text\"] = texts.apply(clean_text)\n",
        "\n",
        "print(df[\"cleaned_text\"].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def nltk_tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
        "    return tokens\n",
        "\n",
        "df[\"tokens_nltk\"] = df[\"cleaned_text\"].apply(nltk_tokenize)\n",
        "\n",
        "all_tokens = [token for tokens in df[\"tokens_nltk\"] for token in tokens]\n",
        "freq_dist = Counter(all_tokens).most_common(10)\n",
        "\n",
        "print(\"Top 10 frequent tokens (NLTK):\")\n",
        "print(freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa-99ZUAGePy",
        "outputId": "b2d64d2a-b94d-4e26-c064-cbc87130dc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 frequent tokens (NLTK):\n",
            "[('says', 4561), ('world', 2030), ('bbc', 2011), ('people', 1989), ('england', 1922), ('first', 1905), ('new', 1894), ('say', 1676), ('cup', 1486), ('two', 1392)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def spacy_process(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_.lower() for token in doc\n",
        "              if not token.is_stop and token.is_alpha and len(token) > 2]\n",
        "    stems = [stemmer.stem(token) for token in lemmas]\n",
        "    return lemmas, stems\n",
        "\n",
        "df[\"lemmas\"], df[\"stems\"] = zip(*df[\"cleaned_text\"].apply(spacy_process))\n",
        "\n",
        "all_lemmas = [lemma for lemmas in df[\"lemmas\"] for lemma in lemmas]\n",
        "lemma_freq = Counter(all_lemmas).most_common(10)\n",
        "\n",
        "print(\"Top 10 frequent lemmas (spaCy):\")\n",
        "print(lemma_freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yymQBFHFHoVa",
        "outputId": "2c371864-8f14-438a-fe0f-d0487d3b1fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 frequent lemmas (spaCy):\n",
            "[('say', 5463), ('england', 2313), ('year', 2250), ('world', 2170), ('bbc', 2026), ('people', 2021), ('win', 1918), ('new', 1898), ('cup', 1501), ('day', 1500)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxREPTiFGeTL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}